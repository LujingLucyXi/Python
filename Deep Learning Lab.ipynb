{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Lab\n",
    "# Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simeng Sun, Saisanthosh Mamidala, Ryan Murtha, Shuyu Sui, Lujing Xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame= cap.read()   # Forever it returns the frame and ret which is false or true\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #if you want to convert the color\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('gray', gray)   # to show the gray video\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):   # If q is pressed stop\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "base_dir = r'C:\\Users\\Sawyer\\Desktop\\Python notebook\\Simeng\\ANLY 535\\Lab3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1- Try to understand each line of the code and explain it in your report.\n",
    "\n",
    "#### ANSWER: \n",
    "#### First import package and initalize an instance of cv.VideoCapture.\n",
    "#### Untilll key \"q\" is preseed, camera will keep reading input image from camera and display a colorful and a black/white version of the orginal image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/12\n",
      "375/375 [==============================] - 32s 86ms/step - loss: 0.2680 - accuracy: 0.9173 - val_loss: 0.0650 - val_accuracy: 0.9827\n",
      "Epoch 2/12\n",
      "375/375 [==============================] - 34s 90ms/step - loss: 0.0919 - accuracy: 0.9721 - val_loss: 0.0455 - val_accuracy: 0.9872\n",
      "Epoch 3/12\n",
      "375/375 [==============================] - 35s 93ms/step - loss: 0.0686 - accuracy: 0.9794 - val_loss: 0.0458 - val_accuracy: 0.9866\n",
      "Epoch 4/12\n",
      "375/375 [==============================] - 33s 89ms/step - loss: 0.0535 - accuracy: 0.9831 - val_loss: 0.0394 - val_accuracy: 0.9898\n",
      "Epoch 5/12\n",
      "375/375 [==============================] - 34s 90ms/step - loss: 0.0465 - accuracy: 0.9855 - val_loss: 0.0388 - val_accuracy: 0.9892\n",
      "Epoch 6/12\n",
      "375/375 [==============================] - 33s 89ms/step - loss: 0.0396 - accuracy: 0.9875 - val_loss: 0.0390 - val_accuracy: 0.9888\n",
      "Epoch 7/12\n",
      "375/375 [==============================] - 34s 89ms/step - loss: 0.0364 - accuracy: 0.9884 - val_loss: 0.0424 - val_accuracy: 0.9886\n",
      "Epoch 8/12\n",
      "375/375 [==============================] - 38s 100ms/step - loss: 0.0329 - accuracy: 0.9894 - val_loss: 0.0374 - val_accuracy: 0.9900\n",
      "Epoch 9/12\n",
      "375/375 [==============================] - 38s 101ms/step - loss: 0.0293 - accuracy: 0.9903 - val_loss: 0.0405 - val_accuracy: 0.9897\n",
      "Epoch 10/12\n",
      "375/375 [==============================] - 35s 92ms/step - loss: 0.0274 - accuracy: 0.9910 - val_loss: 0.0391 - val_accuracy: 0.9903\n",
      "Epoch 11/12\n",
      "375/375 [==============================] - 33s 88ms/step - loss: 0.0257 - accuracy: 0.9918 - val_loss: 0.0429 - val_accuracy: 0.9894\n",
      "Epoch 12/12\n",
      "375/375 [==============================] - 33s 89ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.0419 - val_accuracy: 0.9900\n",
      "Test loss: 0.028563983738422394\n",
      "Test accuracy: 0.9933000206947327\n"
     ]
    }
   ],
   "source": [
    "# To train the model:\n",
    "%run \"C:/Users/Sawyer/Desktop/Python notebook/Simeng/ANLY 535/Lab3/mnist_cnn\"\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_img_contour_thresh(img):\n",
    "#     x, y, w, h = 0, 0, 300, 300\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     blur = cv2.GaussianBlur(gray, (35, 35), 0)\n",
    "#     ret, thresh1 = cv2.threshold(blur, 70, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "#     thresh1 = thresh1[y:y + h, x:x + w]\n",
    "#     contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "#     return img, contours, thresh1\n",
    "def get_img_contour_thresh(img):\n",
    "    x, y, w, h = 0, 0, 300, 300\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (35, 35), 0)\n",
    "    ret, thresh1 = cv2.threshold(blur, 70, 255, cv2.THRESH_BINARY_INV +cv2.THRESH_OTSU)\n",
    "    thresh1 = thresh1[y:y + h, x:x + w]\n",
    "    contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "    return img, contours, thresh1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import tensorflow\n",
    "new_model = tensorflow.keras.models.load_model(os.path.join(base_dir, 'my_model.h5'))\n",
    "# Handwritten recognition using camera\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, img = cap.read()\n",
    "    ret\n",
    "    img, contours, thresh = get_img_contour_thresh(img)\n",
    "    ans1 = ''\n",
    "    if len(contours) > 0:\n",
    "        contour = max(contours, key=cv2.contourArea)\n",
    "        if cv2.contourArea(contour) > 2500:\n",
    "            # print(predict(w_from_model,b_from_model,contour))\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            # newImage = thresh[y - 15:y + h + 15, x - 15:x + w +15]\n",
    "            newImage = thresh[y:y + h, x:x + w]\n",
    "            newImage = cv2.resize(newImage, (28, 28))\n",
    "            newImage = np.array(newImage)\n",
    "            newImage = newImage.flatten()\n",
    "            newImage = newImage.reshape(newImage.shape[0], 1)\n",
    "            newImage2 = newImage.flatten().reshape(1,28,28,1)\n",
    "            newImage2 = newImage2.astype('float32')\n",
    "            newImage2 /= 255\n",
    "            result = new_model.predict(newImage2)\n",
    "            ans1= np.argmax(result)\n",
    "            #ans1 = Digit_Recognizer_LR.predict(w_LR, b_LR, newImage)\n",
    "\n",
    "\n",
    "\n",
    "    x, y, w, h = 0, 0, 300, 300\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(img, \"Prediction : \" + str(ans1), (10, 320),  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", img)\n",
    "    cv2.imshow(\"Contours\", thresh)\n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1.3\n",
    "import cv2\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(os.path.join(base_dir, 'haarcascade_frontalface_default.xml'))\n",
    "eye_cascade = cv2.CascadeClassifier(os.path.join(base_dir, 'haarcascade_eye.xml'))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = img[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "            for (ex,ey,ew,eh) in eyes:\n",
    "                cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "    cv2.imshow('Lab 3 Face recognition',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
